{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":12923148,"sourceType":"datasetVersion","datasetId":8177404},{"sourceId":12923155,"sourceType":"datasetVersion","datasetId":8177410}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:12:26.786435Z","iopub.execute_input":"2025-09-06T22:12:26.786690Z","iopub.status.idle":"2025-09-06T22:12:27.738330Z","shell.execute_reply.started":"2025-09-06T22:12:26.786663Z","shell.execute_reply":"2025-09-06T22:12:27.737533Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cmi-model-3-label-encoder/label_encoder.pkl\n/kaggle/input/cmi-model-3-utilities/train_universe.csv\n/kaggle/input/cmi-model-3-utilities/best_fold3.pt\n/kaggle/input/cmi-model-3-utilities/best_fold2.pt\n/kaggle/input/cmi-model-3-utilities/best_fold0.pt\n/kaggle/input/cmi-model-3-utilities/best_fold1.pt\n/kaggle/input/cmi-model-3-utilities/best_fold4.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport kagglehub\nfrom pathlib import Path\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.spatial.transform import Rotation as R\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom tqdm.notebook import tqdm\nfrom torch.amp import autocast\nimport pandas as pd\nimport polars as pl\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom transformers import BertConfig, BertModel\nimport time\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:12:27.740370Z","iopub.execute_input":"2025-09-06T22:12:27.740777Z","iopub.status.idle":"2025-09-06T22:13:00.219686Z","shell.execute_reply.started":"2025-09-06T22:12:27.740758Z","shell.execute_reply":"2025-09-06T22:13:00.218337Z"}},"outputs":[{"name":"stderr","text":"2025-09-06 22:12:45.500167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757196765.744006      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757196765.815645      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data_path = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:13:00.223419Z","iopub.execute_input":"2025-09-06T22:13:00.223727Z","iopub.status.idle":"2025-09-06T22:13:00.228443Z","shell.execute_reply.started":"2025-09-06T22:13:00.223699Z","shell.execute_reply":"2025-09-06T22:13:00.227656Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **Steps to Convert Raw Dataset â†’ Required Format**","metadata":{}},{"cell_type":"code","source":"\ndef remove_gravity_from_acc(acc_data, rot_data):\n    \"\"\"\n    acc_data: DataFrame or ndarray with columns ['acc_x','acc_y','acc_z']\n    rot_data: DataFrame or ndarray with columns ['rot_x','rot_y','rot_z','rot_w']\n    Returns ndarray (N,3) linear acceleration (gravity removed) in sensor frame.\n    \"\"\"\n    if isinstance(acc_data, pd.DataFrame):\n        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values.astype(float)\n    else:\n        acc_values = np.asarray(acc_data, dtype=float)\n\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values.astype(float)\n    else:\n        quat_values = np.asarray(rot_data, dtype=float)\n\n    num_samples = acc_values.shape[0]\n    linear_accel = np.zeros_like(acc_values)\n    gravity_world = np.array([0.0, 0.0, 9.81], dtype=float)\n\n    for i in range(num_samples):\n        q = quat_values[i]\n        if q is None or np.any(np.isnan(q)) or np.allclose(q, 0.0):\n            linear_accel[i, :] = acc_values[i, :]\n            continue\n        try:\n            rotation = R.from_quat(q)\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except Exception:\n            linear_accel[i, :] = acc_values[i, :]\n    return linear_accel\n\n\ndef angular_velocity_from_quat_with_dt(quat_array, times=None, default_fs=200.0):\n    \"\"\"\n    quat_array: (N,4) array in order (rot_x,rot_y,rot_z,rot_w)\n    times: optional (N,) timestamps in seconds (or ms; function will normalize)\n    default_fs: fallback sampling rate in Hz\n    Returns angular_vel (N,3) where last row is zero.\n    \"\"\"\n    quat_array = np.asarray(quat_array, dtype=float)\n    N = quat_array.shape[0]\n    ang_vel = np.zeros((N, 3), dtype=float)\n    if N < 2:\n        return ang_vel\n\n    if times is not None:\n        t = np.asarray(times, dtype=float)\n        # heuristic: if timestamps look like epoch ms -> convert\n        if np.nanmean(t) > 1e6:\n            t = t / 1000.0\n        dt = np.diff(t)\n        # handle non-positive dt\n        pos = dt[dt > 0]\n        if pos.size == 0:\n            median_dt = 1.0 / default_fs\n        else:\n            median_dt = np.median(pos)\n        dt = np.where(dt > 0, dt, median_dt)\n    else:\n        dt = np.full(N-1, 1.0 / default_fs)\n\n    for i in range(N-1):\n        q_t = quat_array[i]\n        q_tp1 = quat_array[i+1]\n        if (np.any(np.isnan(q_t)) or np.allclose(q_t, 0.0)) or \\\n           (np.any(np.isnan(q_tp1)) or np.allclose(q_tp1, 0.0)):\n            continue\n        try:\n            r_t = R.from_quat(q_t)\n            r_tp1 = R.from_quat(q_tp1)\n            delta = r_t.inv() * r_tp1\n            rotvec = delta.as_rotvec()\n            ang_vel[i, :] = rotvec / dt[i]\n        except Exception:\n            # keep zeros on failure\n            pass\n    return ang_vel\n\n\ndef calculate_angular_distance_with_times(quat_array, times=None):\n    \"\"\"\n    Compute per-sample angular distance (angle between consecutive orientations).\n    Returns array shape (N,) with last element 0.\n    \"\"\"\n    quat_array = np.asarray(quat_array, dtype=float)\n    N = quat_array.shape[0]\n    ang_dist = np.zeros(N, dtype=float)\n    if N < 2:\n        return ang_dist\n\n    for i in range(N-1):\n        q1 = quat_array[i]\n        q2 = quat_array[i+1]\n        if (np.any(np.isnan(q1)) or np.allclose(q1, 0.0)) or \\\n           (np.any(np.isnan(q2)) or np.allclose(q2, 0.0)):\n            ang_dist[i] = 0.0\n            continue\n        try:\n            r1 = R.from_quat(q1)\n            r2 = R.from_quat(q2)\n            relative = r1.inv() * r2\n            ang_dist[i] = np.linalg.norm(relative.as_rotvec())\n        except Exception:\n            ang_dist[i] = 0.0\n    return ang_dist\n\n\ndef compute_tof_region_stats_matrix(subdf_mat, modes):\n    \"\"\"\n    subdf_mat: numpy array shape (N,64) with NaNs for missing values\n    modes: list of ints, e.g. [4] or [2,4,8,16,32]\n    Returns dict mapping column_name -> ndarray (N,)\n    \"\"\"\n    new_cols = {}\n    mat = subdf_mat.astype(float)  # (N,64)\n    with np.errstate(invalid='ignore'):\n        new_cols[\"mean\"] = np.nanmean(mat, axis=1)\n        new_cols[\"std\"]  = np.nanstd(mat, axis=1)\n        new_cols[\"min\"]  = np.nanmin(mat, axis=1)\n        new_cols[\"max\"]  = np.nanmax(mat, axis=1)\n\n    flat = mat.reshape(-1, 64)\n    for mode in modes:\n        if mode <= 0:\n            continue\n        region_size = 64 // mode if mode>0 and 64%mode==0 else max(1, 64 // mode)\n        for r in range(mode):\n            start = r * region_size\n            end = start + region_size if (r < mode-1) else 64\n            region_vals = flat[:, start:end]\n            with np.errstate(all='ignore'):\n                mean_v = np.nanmean(region_vals, axis=1)\n                std_v  = np.nanstd(region_vals, axis=1)\n                min_v  = np.nanmin(region_vals, axis=1)\n                max_v  = np.nanmax(region_vals, axis=1)\n            # keys will be filled by caller with tofid prefix\n            new_cols[f\"r{mode}_{r}_mean\"] = mean_v\n            new_cols[f\"r{mode}_{r}_std\"]  = std_v\n            new_cols[f\"r{mode}_{r}_min\"]  = min_v\n            new_cols[f\"r{mode}_{r}_max\"]  = max_v\n    return new_cols\n\n\ndef add_all_tof_features_vectorized(df, tof_mode):\n    \"\"\"\n    For each tof sensor 1..5, compute:\n      tof_{i}_mean/std/min/max\n      and for each mode in modes: tof{mode}_{i}_region_{r}_{stat}\n    Returns a new DataFrame = pd.concat([df, new_cols_df], axis=1)\n    \"\"\"\n    if tof_mode == 0:\n        modes = []\n    elif tof_mode == -1:\n        modes = [2,4,8,16,32]\n    else:\n        modes = [tof_mode]\n\n    all_new = {}\n    N = len(df)\n    for tof_id in range(1, 6):\n        tof_cols = [f\"tof_{tof_id}_v{p}\" for p in range(64)]\n        # ensure columns exist (create NaN if missing)\n        for c in tof_cols:\n            if c not in df.columns:\n                df[c] = np.nan\n\n        subdf = df[tof_cols].astype(float).replace(-1, np.nan)\n        mat = subdf.values  # (N,64)\n        new = compute_tof_region_stats_matrix(mat, modes)\n        # prefix names and add to all_new\n        all_new[f\"tof_{tof_id}_mean\"] = new[\"mean\"]\n        all_new[f\"tof_{tof_id}_std\"]  = new[\"std\"]\n        all_new[f\"tof_{tof_id}_min\"]  = new[\"min\"]\n        all_new[f\"tof_{tof_id}_max\"]  = new[\"max\"]\n        for mode in modes:\n            for r in range(mode):\n                all_new[f\"tof{mode}_{tof_id}_region_{r}_mean\"] = new[f\"r{mode}_{r}_mean\"]\n                all_new[f\"tof{mode}_{tof_id}_region_{r}_std\"]  = new[f\"r{mode}_{r}_std\"]\n                all_new[f\"tof{mode}_{tof_id}_region_{r}_min\"]  = new[f\"r{mode}_{r}_min\"]\n                all_new[f\"tof{mode}_{tof_id}_region_{r}_max\"]  = new[f\"r{mode}_{r}_max\"]\n\n    # concat once\n    new_df = pd.concat([df, pd.DataFrame(all_new, index=df.index)], axis=1)\n    return new_df\n\n# -------------------------\n# Utility: estimate per-sequence timestamps (seconds)\n# -------------------------\ndef get_sequence_times(group_df, time_col='timestamp'):\n    \"\"\"\n    Returns times array in seconds if available, else None.\n    If timestamps appear to be epoch-ms, convert to seconds.\n    \"\"\"\n    if time_col in group_df.columns:\n        times = group_df[time_col].astype(float).values\n        if np.nanmean(times) > 1e6:\n            times = times / 1000.0\n        return times\n    return None\n\n# -------------------------\n# Main pipeline\n# -------------------------\ndef make_universe_csv(raw_csv_path,\n                      out_csv_path=\"universe.csv\",\n                      tof_mode=16,\n                      default_fs=200.0,\n                      time_col='timestamp',\n                      tof_raw_keep=True):\n    \"\"\"\n    raw_csv_path: path to raw csv (one row per timestamp). Must include sequence_id and sensor columns.\n    out_csv_path: final CSV file for CMIFeDataset\n    tof_mode: same behavior as CMIFeDataset (0, >1, or -1)\n    default_fs: fallback sampling rate for angular velocity if no timestamps present\n    time_col: name of timestamp column if present (optional)\n    tof_raw_keep: if True keep raw tof_*_v* columns (recommended)\n    \"\"\"\n    print(\"Loading raw CSV:\", raw_csv_path)\n    df_raw = pd.read_csv(raw_csv_path)\n    print(\"Raw shape:\", df_raw.shape)\n\n    # Prepare base: ensure sequence_id present\n    if 'sequence_id' not in df_raw.columns:\n        raise ValueError(\"Raw CSV must contain 'sequence_id' column.\")\n\n    df = df_raw.copy()\n\n    # ensure base columns exist to avoid KeyError downstream\n    for col in ['acc_x','acc_y','acc_z','rot_x','rot_y','rot_z','rot_w']:\n        if col not in df.columns:\n            df[col] = np.nan\n    # ensure thermal columns exist: thm_1..thm_5\n    for i in range(1,6):\n        c = f\"thm_{i}\"\n        if c not in df.columns:\n            df[c] = np.nan\n\n    # ----- IMU: acc_mag, rot_angle (vectorized) -----\n    with np.errstate(invalid='ignore'):\n        df['acc_mag'] = np.sqrt(df['acc_x'].astype(float)**2 + df['acc_y'].astype(float)**2 + df['acc_z'].astype(float)**2)\n        df['rot_angle'] = 2 * np.arccos(df['rot_w'].astype(float).clip(-1,1))\n\n    # compute acc_mag_jerk and rot_angle_vel by group (vectorized groupby diff)\n    df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n    df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n\n    # ----- Linear accel (remove gravity) per sequence -----\n    N = len(df)\n    lin_x = np.full(N, np.nan, dtype=float)\n    lin_y = np.full(N, np.nan, dtype=float)\n    lin_z = np.full(N, np.nan, dtype=float)\n\n    if all(c in df.columns for c in ['acc_x','acc_y','acc_z','rot_x','rot_y','rot_z','rot_w']):\n        print(\"Computing gravity-removed linear acceleration per sequence...\")\n        for seq_id, group in tqdm(df.groupby('sequence_id'), desc=\"linear_acc\"):\n            idx = group.index\n            acc_sub = group[['acc_x','acc_y','acc_z']].astype(float)\n            rot_sub = group[['rot_x','rot_y','rot_z','rot_w']].astype(float)\n            la = remove_gravity_from_acc(acc_sub, rot_sub)\n            lin_x[idx] = la[:,0]\n            lin_y[idx] = la[:,1]\n            lin_z[idx] = la[:,2]\n    else:\n        warnings.warn(\"Rotation or accelerometer columns missing; using raw acc with gravity approx.\")\n        if 'acc_x' in df.columns:\n            lin_x[:] = df['acc_x'].astype(float).fillna(0.0)\n        if 'acc_y' in df.columns:\n            lin_y[:] = df['acc_y'].astype(float).fillna(0.0)\n        if 'acc_z' in df.columns:\n            lin_z[:] = df['acc_z'].astype(float).fillna(0.0) - 9.81\n\n    df['linear_acc_x'] = lin_x\n    df['linear_acc_y'] = lin_y\n    df['linear_acc_z'] = lin_z\n    df['linear_acc_mag'] = np.sqrt(np.nan_to_num(df['linear_acc_x'])**2 + np.nan_to_num(df['linear_acc_y'])**2 + np.nan_to_num(df['linear_acc_z'])**2)\n    df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n\n    # ----- Angular velocity & angular distance by sequence using timestamps if available -----\n    N = len(df)\n    av_x = np.zeros(N, dtype=float)\n    av_y = np.zeros(N, dtype=float)\n    av_z = np.zeros(N, dtype=float)\n    ang_dist = np.zeros(N, dtype=float)\n\n    if all(c in df.columns for c in ['rot_x','rot_y','rot_z','rot_w']):\n        print(\"Computing angular velocity & angular distance per sequence...\")\n        for seq_id, group in tqdm(df.groupby('sequence_id'), desc=\"ang_vel\"):\n            idx = group.index\n            quat_arr = group[['rot_x','rot_y','rot_z','rot_w']].astype(float).values\n            times = get_sequence_times(group, time_col=time_col)  # None if missing\n            ang_vel = angular_velocity_from_quat_with_dt(quat_arr, times=times, default_fs=default_fs)\n            ad = calculate_angular_distance_with_times(quat_arr, times=times)\n            av_x[idx] = ang_vel[:,0]\n            av_y[idx] = ang_vel[:,1]\n            av_z[idx] = ang_vel[:,2]\n            ang_dist[idx] = ad\n    else:\n        warnings.warn(\"Rotation quaternion columns missing -> angular velocity/distance set to 0.\")\n\n    df['angular_vel_x'] = av_x\n    df['angular_vel_y'] = av_y\n    df['angular_vel_z'] = av_z\n    df['angular_distance'] = ang_dist\n\n    # Fill any remaining NaNs in engineered IMU with zeros (ensures columns exist)\n    imu_engineered = ['acc_mag','rot_angle','acc_mag_jerk','rot_angle_vel',\n                      'linear_acc_x','linear_acc_y','linear_acc_z',\n                      'linear_acc_mag','linear_acc_mag_jerk',\n                      'angular_vel_x','angular_vel_y','angular_vel_z',\n                      'angular_distance']\n    for c in imu_engineered:\n        if c not in df.columns:\n            df[c] = 0.0\n        else:\n            df[c] = df[c].fillna(0.0)\n\n    # ----- Thermal basic stats (optional) -----\n    thm_cols = [f\"thm_{i}\" for i in range(1,6)]\n    present_thm = [c for c in thm_cols if c in df.columns]\n    if present_thm:\n        df['thm_mean'] = df[present_thm].astype(float).mean(axis=1)\n        df['thm_std']  = df[present_thm].astype(float).std(axis=1).fillna(0.0)\n    else:\n        warnings.warn(\"Thermal columns thm_1..thm_5 not found.\")\n\n    # ----- ToF features (vectorized, concat once) -----\n    print(\"Computing ToF features (this may take a moment for large files)...\")\n    df = add_all_tof_features_vectorized(df, tof_mode)\n\n    # Optionally drop raw tof columns (not recommended for baseline)\n    if not tof_raw_keep:\n        for tof_id in range(1,6):\n            for p in range(64):\n                c = f\"tof_{tof_id}_v{p}\"\n                if c in df.columns:\n                    df.drop(columns=[c], inplace=True)\n\n    # Final save\n    print(\"Final dataframe shape:\", df.shape)\n    df.to_csv(out_csv_path, index=False)\n    print(\"Saved universe CSV to:\", out_csv_path)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:13:00.229454Z","iopub.execute_input":"2025-09-06T22:13:00.229945Z","iopub.status.idle":"2025-09-06T22:13:00.273648Z","shell.execute_reply.started":"2025-09-06T22:13:00.229906Z","shell.execute_reply":"2025-09-06T22:13:00.272714Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\": \n    RAW_CSV = data_path      \n    OUT_CSV = \"/kaggle/working/train_universe.csv\"     \n    TOF_MODE = 16                  # set to same value your dataset config uses (e.g. 4,8,16) or -1 for [2,4,8,16,32]\n    DEFAULT_FS = 200.0             \n\n    # run\n    universe_df = make_universe_csv(RAW_CSV, OUT_CSV, tof_mode=TOF_MODE, default_fs=DEFAULT_FS, time_col='timestamp', tof_raw_keep=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:13:21.456187Z","iopub.execute_input":"2025-09-06T22:13:21.456621Z","iopub.status.idle":"2025-09-06T22:23:53.631902Z","shell.execute_reply.started":"2025-09-06T22:13:21.456588Z","shell.execute_reply":"2025-09-06T22:23:53.630074Z"}},"outputs":[{"name":"stdout","text":"Loading raw CSV: /kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\nRaw shape: (574945, 341)\nComputing gravity-removed linear acceleration per sequence...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"linear_acc:   0%|          | 0/8151 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405f4085c84b477c9f7f18487f6a03b7"}},"metadata":{}},{"name":"stdout","text":"Computing angular velocity & angular distance per sequence...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ang_vel:   0%|          | 0/8151 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"476b9657f86e46018df4d0c5fc644ed6"}},"metadata":{}},{"name":"stdout","text":"Computing ToF features (this may take a moment for large files)...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/270013593.py:119: RuntimeWarning: Mean of empty slice\n  new_cols[\"mean\"] = np.nanmean(mat, axis=1)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/tmp/ipykernel_36/270013593.py:121: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"min\"]  = np.nanmin(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:122: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"max\"]  = np.nanmax(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:134: RuntimeWarning: Mean of empty slice\n  mean_v = np.nanmean(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:136: RuntimeWarning: All-NaN slice encountered\n  min_v  = np.nanmin(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:137: RuntimeWarning: All-NaN slice encountered\n  max_v  = np.nanmax(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:119: RuntimeWarning: Mean of empty slice\n  new_cols[\"mean\"] = np.nanmean(mat, axis=1)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/tmp/ipykernel_36/270013593.py:121: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"min\"]  = np.nanmin(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:122: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"max\"]  = np.nanmax(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:134: RuntimeWarning: Mean of empty slice\n  mean_v = np.nanmean(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:136: RuntimeWarning: All-NaN slice encountered\n  min_v  = np.nanmin(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:137: RuntimeWarning: All-NaN slice encountered\n  max_v  = np.nanmax(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:119: RuntimeWarning: Mean of empty slice\n  new_cols[\"mean\"] = np.nanmean(mat, axis=1)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/tmp/ipykernel_36/270013593.py:121: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"min\"]  = np.nanmin(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:122: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"max\"]  = np.nanmax(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:134: RuntimeWarning: Mean of empty slice\n  mean_v = np.nanmean(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:136: RuntimeWarning: All-NaN slice encountered\n  min_v  = np.nanmin(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:137: RuntimeWarning: All-NaN slice encountered\n  max_v  = np.nanmax(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:119: RuntimeWarning: Mean of empty slice\n  new_cols[\"mean\"] = np.nanmean(mat, axis=1)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/tmp/ipykernel_36/270013593.py:121: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"min\"]  = np.nanmin(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:122: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"max\"]  = np.nanmax(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:134: RuntimeWarning: Mean of empty slice\n  mean_v = np.nanmean(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:136: RuntimeWarning: All-NaN slice encountered\n  min_v  = np.nanmin(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:137: RuntimeWarning: All-NaN slice encountered\n  max_v  = np.nanmax(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:119: RuntimeWarning: Mean of empty slice\n  new_cols[\"mean\"] = np.nanmean(mat, axis=1)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/tmp/ipykernel_36/270013593.py:121: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"min\"]  = np.nanmin(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:122: RuntimeWarning: All-NaN slice encountered\n  new_cols[\"max\"]  = np.nanmax(mat, axis=1)\n/tmp/ipykernel_36/270013593.py:134: RuntimeWarning: Mean of empty slice\n  mean_v = np.nanmean(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:136: RuntimeWarning: All-NaN slice encountered\n  min_v  = np.nanmin(region_vals, axis=1)\n/tmp/ipykernel_36/270013593.py:137: RuntimeWarning: All-NaN slice encountered\n  max_v  = np.nanmax(region_vals, axis=1)\n","output_type":"stream"},{"name":"stdout","text":"Final dataframe shape: (574945, 696)\nSaved universe CSV to: /kaggle/working/train_universe.csv\n","output_type":"stream"}],"execution_count":6}]}