{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":240649816,"sourceType":"kernelVersion"},{"sourceId":245926974,"sourceType":"kernelVersion"},{"sourceId":251413288,"sourceType":"kernelVersion"},{"sourceId":470587,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":379625,"modelId":398856},{"sourceId":471764,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":380358,"modelId":400086}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":265.218222,"end_time":"2025-09-01T14:28:33.758213","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-01T14:24:08.539991","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"88d4a549-d24a-4fbe-a1eb-406ca6f21c74","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:04.626361Z","iopub.execute_input":"2025-09-06T21:58:04.626520Z","iopub.status.idle":"2025-09-06T21:58:07.381565Z","shell.execute_reply.started":"2025-09-06T21:58:04.626504Z","shell.execute_reply":"2025-09-06T21:58:07.380717Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cmi-precompute/pytorch/all/1/tof2_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof16_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof0_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof4_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof8_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof32_raw.csv\n/kaggle/input/cmi-precompute/pytorch/all/1/tof1_raw.csv\n/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1/fold1/best_ema.pt\n/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1/fold3/best_ema.pt\n/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1/fold0/best_ema.pt\n/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1/fold4/best_ema.pt\n/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1/fold2/best_ema.pt\n/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/cmi_inference_server.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/cmi_gateway.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/__init__.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/templates.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/relay.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/__init__.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/__init__.py\n/kaggle/input/cmi-metric/__results__.html\n/kaggle/input/cmi-metric/__package_validation_results__.txt\n/kaggle/input/cmi-metric/__notebook__.ipynb\n/kaggle/input/cmi-metric/__output__.json\n/kaggle/input/cmi-metric/custom.css\n/kaggle/input/cmi-metric/package/kagglehub_requirements.yaml\n/kaggle/input/cmi-metric/package/core.py\n/kaggle/input/cmi-metric/package/__init__.py\n/kaggle/input/deterministic/__results__.html\n/kaggle/input/deterministic/__package_validation_results__.txt\n/kaggle/input/deterministic/__notebook__.ipynb\n/kaggle/input/deterministic/__output__.json\n/kaggle/input/deterministic/custom.css\n/kaggle/input/deterministic/package/kagglehub_requirements.yaml\n/kaggle/input/deterministic/package/core.py\n/kaggle/input/deterministic/package/__init__.py\n","output_type":"stream"}],"execution_count":1},{"id":"a6fbac2f-d262-4c4b-927a-8577e5d00c50","cell_type":"markdown","source":"# **Libraries**","metadata":{}},{"id":"a291778f","cell_type":"code","source":"import os\nimport torch\nimport kagglehub\nfrom pathlib import Path\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.spatial.transform import Rotation as R\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom tqdm.notebook import tqdm\nfrom torch.amp import autocast\nimport pandas as pd\nimport polars as pl\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom transformers import BertConfig, BertModel\nimport math\nimport time\nimport joblib","metadata":{"_cell_guid":"80dac13f-ca07-4786-b31d-0f0c72694ebe","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8e6b8a61-6c7d-4277-9465-f4b25b02c4f1","execution":{"iopub.status.busy":"2025-09-06T21:58:07.382388Z","iopub.execute_input":"2025-09-06T21:58:07.382693Z","iopub.status.idle":"2025-09-06T21:58:43.902777Z","shell.execute_reply.started":"2025-09-06T21:58:07.382676Z","shell.execute_reply":"2025-09-06T21:58:43.902171Z"},"papermill":{"duration":153.947881,"end_time":"2025-09-01T14:27:24.141325","exception":false,"start_time":"2025-09-01T14:24:50.193444","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2025-09-06 21:58:28.449531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757195908.802092      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757195908.927166      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"ddc4ab5d-d15a-4642-9ded-dccd3f5a099b","cell_type":"markdown","source":"# **Preprocessing Functions**","metadata":{}},{"id":"43c22c2c-cd36-4add-b192-54590bad441b","cell_type":"code","source":"# Removing the gravity\ndef remove_gravity_from_acc(acc_data, rot_data):\n    if isinstance(acc_data, pd.DataFrame):\n        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n    else:\n        acc_values = acc_data\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n    num_samples = acc_values.shape[0]\n    linear_accel = np.zeros_like(acc_values)\n    gravity_world = np.array([0, 0, 9.81])\n    for i in range(num_samples):\n        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n            linear_accel[i, :] = acc_values[i, :] \n            continue\n        try:\n            rotation = R.from_quat(quat_values[i])\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except ValueError:\n             linear_accel[i, :] = acc_values[i, :]\n    return linear_accel\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:43.904367Z","iopub.execute_input":"2025-09-06T21:58:43.904846Z","iopub.status.idle":"2025-09-06T21:58:43.911211Z","shell.execute_reply.started":"2025-09-06T21:58:43.904825Z","shell.execute_reply":"2025-09-06T21:58:43.910410Z"}},"outputs":[],"execution_count":3},{"id":"b1d19c0e-c32a-48fc-98b0-c4e8d09b91c2","cell_type":"code","source":"# Calclate angular velocity\ndef calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n    num_samples = quat_values.shape[0]\n    angular_vel = np.zeros((num_samples, 3))\n    for i in range(num_samples - 1):\n        q_t = quat_values[i]\n        q_t_plus_dt = quat_values[i+1]\n        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n            continue\n        try:\n            rot_t = R.from_quat(q_t)\n            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n            delta_rot = rot_t.inv() * rot_t_plus_dt\n            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n        except ValueError:\n            pass\n    return angular_vel\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:43.912155Z","iopub.execute_input":"2025-09-06T21:58:43.912471Z","iopub.status.idle":"2025-09-06T21:58:43.949868Z","shell.execute_reply.started":"2025-09-06T21:58:43.912447Z","shell.execute_reply":"2025-09-06T21:58:43.949315Z"}},"outputs":[],"execution_count":4},{"id":"f3e91a13-451c-410c-b546-b35221f66188","cell_type":"code","source":"# Calculate Angular Distance\ndef calculate_angular_distance(rot_data):\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n    num_samples = quat_values.shape[0]\n    angular_dist = np.zeros(num_samples)\n    for i in range(num_samples - 1):\n        q1 = quat_values[i]\n        q2 = quat_values[i+1]\n        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n            angular_dist[i] = 0\n            continue\n        try:\n            r1 = R.from_quat(q1)\n            r2 = R.from_quat(q2)\n            relative_rotation = r1.inv() * r2\n            angle = np.linalg.norm(relative_rotation.as_rotvec())\n            angular_dist[i] = angle\n        except ValueError:\n            angular_dist[i] = 0 # In the case of invalid quaternions\n            pass\n    return angular_dist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:43.950590Z","iopub.execute_input":"2025-09-06T21:58:43.950879Z","iopub.status.idle":"2025-09-06T21:58:43.969790Z","shell.execute_reply.started":"2025-09-06T21:58:43.950858Z","shell.execute_reply":"2025-09-06T21:58:43.969255Z"}},"outputs":[],"execution_count":5},{"id":"4277f838-fd67-40e2-8842-dd2e030b9a46","cell_type":"markdown","source":"# **Dataset Classes and Functions**","metadata":{}},{"id":"f11b3783-61e5-4390-b73f-488510b9f56e","cell_type":"markdown","source":"This `CMIFeDataset` class is a **PyTorch Dataset** that:\n\n1. **Loads raw CSV data** (with IMU, thermal, and ToF sensor readings) and **config settings**.\n2. **Initializes feature names** (engineered features for IMU, raw & aggregated stats for ToF, etc.).\n3. **Generates or loads engineered features** if they’re not already precomputed:\n\n   * IMU features: magnitudes, jerks, rotation angles, angular velocities/distances, gravity removal, etc.\n   * ToF features: per-sensor statistics (mean, std, min, max) and optionally region-based aggregation.\n4. **Encodes labels** (`gesture`) into integers and one-hot vectors.\n5. **Handles missing data**:\n\n   * Fills with forward/backward fill if configured.\n   * Replaces remaining NaNs with a synthetic “nan value” that is later scaled consistently.\n6. **Scales features** (either all at once or per-sensor type) using `StandardScaler`.\n7. **Pads sequences** to a fixed length (`pad_len`, chosen as the 95th percentile of sequence lengths).\n8. **Stores ready-to-train tensors** for IMU, thermal, and ToF data plus labels & class weights.\n9. Provides:\n\n   * **`__getitem__`** for PyTorch DataLoader compatibility.\n   * **`inference_process`** for preparing new sequences in the exact same way as training data.\n   * Utilities for getting scaled NaN tensors.\n\nBasically — it’s a **complete dataset preparation pipeline** for a multimodal time-series classification task, handling:\n\n* 1) Feature engineering\n* 2) Missing data \n* 3) Normalization\n* 4) Padding\n* 5) Label encoding\n* 6) Train/inference consistency ","metadata":{}},{"id":"908a0785-6f9f-4129-8acb-83b5bbfd4a50","cell_type":"code","source":"# CMIFEDataset Class inheriting base Dataset Class\nclass CMIFeDataset(Dataset):\n    def __init__(self, data_path, config):\n        self.config = config\n        self.init_feature_names(data_path)\n        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.base_cols+self.feature_cols)))\n        self.generate_dataset(df)\n\n    def init_feature_names(self, data_path):\n        self.imu_engineered_features = [\n            'acc_mag', 'rot_angle',\n            'acc_mag_jerk', 'rot_angle_vel',\n            'linear_acc_mag', 'linear_acc_mag_jerk',\n            'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n            'angular_distance'\n        ]\n\n        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n        self.tof_cols = self.generate_tof_feature_names()\n\n        columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n        imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n        imu_cols_base.extend([c for c in columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n        self.imu_cols = list(dict.fromkeys(imu_cols_base + self.imu_engineered_features))\n        self.thm_cols = [c for c in columns if c.startswith('thm_')]\n        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n        self.imu_dim = len(self.imu_cols)\n        self.thm_dim = len(self.thm_cols)\n        self.tof_dim = len(self.tof_cols)\n        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n                          'sequence_id', 'subject', \n                          'sequence_type', 'gesture', 'orientation'] + [c for c in columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation']\n\n    def generate_tof_feature_names(self):\n        features = []\n        if self.config.get(\"tof_raw\", False):\n            for i in range(1, 6):\n                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n        for i in range(1, 6):\n            if self.tof_mode != 0:\n                for stat in self.tof_region_stats:\n                    features.append(f'tof_{i}_{stat}')\n                if self.tof_mode > 1:\n                    for r in range(self.tof_mode):\n                        for stat in self.tof_region_stats:\n                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n                if self.tof_mode == -1:\n                    for mode in [2, 4, 8, 16, 32]:\n                        for r in range(mode):\n                            for stat in self.tof_region_stats:\n                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n        return features\n\n    def compute_features(self, df):\n        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n            \n        linear_accel_list = []\n        for _, group in df.groupby('sequence_id'):\n            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n        df_linear_accel = pd.concat(linear_accel_list)\n        df = pd.concat([df, df_linear_accel], axis=1)\n        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n    \n        angular_vel_list = []\n        for _, group in df.groupby('sequence_id'):\n            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n        df_angular_vel = pd.concat(angular_vel_list)\n        df = pd.concat([df, df_angular_vel], axis=1)\n    \n        angular_distance_list = []\n        for _, group in df.groupby('sequence_id'):\n            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n            angular_dist_group = calculate_angular_distance(rot_data_group)\n            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n        df_angular_distance = pd.concat(angular_distance_list)\n        df = pd.concat([df, df_angular_distance], axis=1)\n\n        if self.tof_mode != 0:\n            new_columns = {}\n            for i in range(1, 6):\n                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n                tof_data = df[pixel_cols].replace(-1, np.nan)\n                new_columns.update({\n                    f'tof_{i}_mean': tof_data.mean(axis=1),\n                    f'tof_{i}_std': tof_data.std(axis=1),\n                    f'tof_{i}_min': tof_data.min(axis=1),\n                    f'tof_{i}_max': tof_data.max(axis=1)\n                })\n                if self.tof_mode > 1:\n                    region_size = 64 // self.tof_mode\n                    for r in range(self.tof_mode):\n                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                        new_columns.update({\n                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                        })\n                if self.tof_mode == -1:\n                    for mode in [2, 4, 8, 16, 32]:\n                        region_size = 64 // mode\n                        for r in range(mode):\n                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                            new_columns.update({\n                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                            })\n            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n        return df\n        \n    def generate_features(self, df):\n        self.le = LabelEncoder()\n        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n        self.class_num = len(self.le.classes_)\n        \n        if all(c in df.columns for c in self.imu_engineered_features) and all(c in df.columns for c in self.tof_cols):\n            print(\"Have precomputed, skip compute.\")\n        else:\n            print(\"Not precomputed, do compute.\")\n            df = self.compute_features(df)\n\n        if self.config.get(\"save_precompute\", False):\n            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n        return df\n\n    def scale(self, data_unscaled):\n        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n        return [scaler.transform(x) for x in data_unscaled], scaler\n\n    def pad(self, data_scaled, cols):\n        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n        for i, seq in enumerate(data_scaled):\n            seq_len = min(len(seq), self.pad_len)\n            pad_data[i, :seq_len] = seq[:seq_len]\n        return pad_data\n\n    def get_nan_value(self, data, ratio):\n        max_value = data.max().max()\n        nan_value = -max_value * ratio\n        return nan_value\n\n    def generate_dataset(self, df):\n        seq_gp = df.groupby('sequence_id') \n        imu_unscaled, thm_unscaled, tof_unscaled = [], [], []\n        imu_mask, thm_mask, tof_mask = [], [], []\n        classes, lens = [], []\n        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n\n        self.fold_feats = defaultdict(list)\n        for seq_id, seq_df in seq_gp:\n            imu_data = seq_df[self.imu_cols]\n            if self.config[\"fbfill\"][\"imu\"]:\n                imu_data = imu_data.ffill().bfill()\n            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n\n            thm_data = seq_df[self.thm_cols]\n            if self.config[\"fbfill\"][\"thm\"]:\n                thm_data = thm_data.ffill().bfill()\n            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n\n            tof_data = seq_df[self.tof_cols]\n            if self.config[\"fbfill\"][\"tof\"]:\n                tof_data = tof_data.ffill().bfill()\n            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n            \n            classes.append(seq_df['gesture_int'].iloc[0])\n            lens.append(len(imu_data))\n\n            for col in self.fold_cols:\n                self.fold_feats[col].append(seq_df[col].iloc[0])\n            \n        self.dataset_indices = classes\n        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n        if self.config.get(\"one_scale\", True):\n            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n            x_scaled, self.x_scaler = self.scale(x_unscaled)\n            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n            self.imu = x[..., :self.imu_dim]\n            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n        else:\n            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n            self.imu = self.pad(imu_scaled, self.imu_cols)\n            self.thm = self.pad(thm_scaled, self.thm_cols)\n            self.tof = self.pad(tof_scaled, self.tof_cols)\n        self.precompute_scaled_nan_values()\n        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n\n    def precompute_scaled_nan_values(self):\n        dummy_df = pd.DataFrame(\n            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n                     [self.thm_nan_value]*len(self.thm_cols) +\n                     [self.tof_nan_value]*len(self.tof_cols)]),\n            columns=self.imu_cols + self.thm_cols + self.tof_cols\n        )\n        \n        if self.config.get(\"one_scale\", True):\n            scaled = self.x_scaler.transform(dummy_df)\n            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n        else:\n            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n\n    def get_scaled_nan_tensors(self, imu, thm, tof):\n        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n\n    def inference_process(self, sequence):\n        df_seq = sequence.to_pandas().copy()\n        if not all(c in df_seq.columns for c in self.imu_engineered_features):\n            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n                linear_accel = remove_gravity_from_acc(\n                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n                )\n                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n            else:\n                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n            else:\n                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n            else:\n                df_seq['angular_distance'] = 0\n\n        if self.tof_mode != 0:\n            new_columns = {} \n            for i in range(1, 6):\n                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n                new_columns.update({\n                    f'tof_{i}_mean': tof_data.mean(axis=1),\n                    f'tof_{i}_std': tof_data.std(axis=1),\n                    f'tof_{i}_min': tof_data.min(axis=1),\n                    f'tof_{i}_max': tof_data.max(axis=1)\n                })\n                if self.tof_mode > 1:\n                    region_size = 64 // self.tof_mode\n                    for r in range(self.tof_mode):\n                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                        new_columns.update({\n                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                        })\n                if self.tof_mode == -1:\n                    for mode in [2, 4, 8, 16, 32]:\n                        region_size = 64 // mode\n                        for r in range(mode):\n                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                            new_columns.update({\n                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                            })\n            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n        \n        imu_unscaled = df_seq[self.imu_cols]\n        if self.config[\"fbfill\"][\"imu\"]:\n            imu_unscaled = imu_unscaled.ffill().bfill()\n        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n\n        thm_unscaled = df_seq[self.thm_cols]\n        if self.config[\"fbfill\"][\"thm\"]:\n            thm_unscaled = thm_unscaled.ffill().bfill()\n        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n\n        tof_unscaled = df_seq[self.tof_cols]\n        if self.config[\"fbfill\"][\"tof\"]:\n            tof_unscaled = tof_unscaled.ffill().bfill()\n        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n        \n        if self.config.get(\"one_scale\", True):\n            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n            x_scaled = self.x_scaler.transform(x_unscaled)\n            imu_scaled = x_scaled[..., :self.imu_dim]\n            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n        else:\n            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n\n        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n        seq_len = min(combined.shape[0], self.pad_len)\n        padded[:seq_len] = combined[:seq_len]\n        imu = padded[..., :self.imu_dim]\n        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n        \n        return torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)\n\n    def __getitem__(self, idx):\n        return self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx]\n\n    def __len__(self):\n        return len(self.class_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:43.970492Z","iopub.execute_input":"2025-09-06T21:58:43.970785Z","iopub.status.idle":"2025-09-06T21:58:44.020489Z","shell.execute_reply.started":"2025-09-06T21:58:43.970757Z","shell.execute_reply":"2025-09-06T21:58:44.019811Z"}},"outputs":[],"execution_count":6},{"id":"6c641e6d-e8f5-43fd-8cf7-ecc9d26bdfbf","cell_type":"markdown","source":"---\n\nThis class is a **cross-validation wrapper** around a full dataset (`CMIFeDataset`).\n\n### What it does:\n\n1. **Initializes**:\n\n   * Builds the full dataset using `full_dataset_function` (e.g., `CMIFeDataset`).\n   * Stores feature dimensions (IMU, thermal, ToF), label encoder, class names, and class weights.\n   * Creates a **StratifiedKFold** object for *n*-fold cross-validation (preserving class balance in each split).\n   * Generates all fold train/validation index splits in advance.\n\n2. **Provides**:\n\n   * `get_fold_datasets(fold_idx)` → returns PyTorch `Subset` objects for training and validation for a given fold.\n   * `print_fold_stats()` → prints how many samples per class are in train/valid sets for each fold (helps verify balanced splits).\n\n---\n\n✅ **Purpose**: Makes it easy to run stratified *n*-fold cross-validation on a dataset while keeping all feature preprocessing identical.\n✅ **Key benefit**: Automatically handles train/validation splitting with label balance and quick class distribution inspection.\n\n---\n","metadata":{}},{"id":"31d05e25-aaf5-4e7f-8e67-33323e03a297","cell_type":"code","source":"# CMIFoldDataset for StratifiedKFold\nclass CMIFoldDataset:\n    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n        self.imu_dim = self.full_dataset.imu_dim\n        self.thm_dim = self.full_dataset.thm_dim\n        self.tof_dim = self.full_dataset.tof_dim\n        self.le = self.full_dataset.le\n        self.class_names = self.full_dataset.le.classes_\n        self.class_weight = self.full_dataset.class_weight\n        all_indices = np.arange(len(self.full_dataset))\n        self.n_folds = n_folds\n        self.skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n        self.folds = list(self.skf.split(all_indices, np.array(self.full_dataset.dataset_indices)))\n    \n    def get_fold_datasets(self, fold_idx):\n        if self.folds is None or fold_idx >= self.n_folds:\n            return None, None\n        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n        return Subset(self.full_dataset, fold_train_idx), Subset(self.full_dataset, fold_valid_idx)\n\n    def print_fold_stats(self):\n        def get_label_counts(subset):\n            counts = {name: 0 for name in self.class_names}\n            if subset is None:\n                return counts\n            for idx in subset.indices:\n                label_idx = self.full_dataset.dataset_indices[idx]\n                counts[self.class_names[label_idx]] += 1\n            return counts\n        \n        print(\"\\nCross-validation fold statistics:\")\n        for fold_idx in range(self.n_folds):\n            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n            train_counts = get_label_counts(train_fold)\n            valid_counts = get_label_counts(valid_fold)\n                \n            print(f\"\\nFold {fold_idx + 1}:\")\n            print(f\"{'Category':<50} {'Training set':<10} {'Validation set':<10}\")\n            for name in self.class_names:\n                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.021251Z","iopub.execute_input":"2025-09-06T21:58:44.021443Z","iopub.status.idle":"2025-09-06T21:58:44.042845Z","shell.execute_reply.started":"2025-09-06T21:58:44.021427Z","shell.execute_reply":"2025-09-06T21:58:44.042090Z"}},"outputs":[],"execution_count":7},{"id":"a6ce245d-0608-4949-a7a0-58e11e173a8f","cell_type":"markdown","source":"Here’s the side-by-side diagram showing how **`CMIFeDataset`** and **`CMIFoldDataset`** relate:\n\n---\n\n**Flow Diagram — Full Pipeline**\n\n```\n                ┌──────────────────────────────────┐\n                │  CMIFeDataset                     │\n                │----------------------------------│\n                │  1. Load CSV (raw sensor data)    │\n                │  2. Init feature names            │\n                │  3. Feature engineering           │\n                │     - IMU engineered features     │\n                │     - ToF aggregated stats        │\n                │  4. Handle missing data           │\n                │  5. Scale & pad sequences         │\n                │  6. Encode labels + weights       │\n                │  7. Store tensors (imu/thm/tof)   │\n                └──────────────────────────────────┘\n                              │\n                              ▼\n        (Full processed dataset: ready for model training)\n                              │\n                              ▼\n                ┌──────────────────────────────────┐\n                │  CMIFoldDataset                   │\n                │----------------------------------│\n                │  1. Takes CMIFeDataset as input   │\n                │  2. Uses StratifiedKFold to split │\n                │     into n folds (balanced)       │\n                │  3. Stores train/valid indices    │\n                │  4. get_fold_datasets(fold_idx)   │\n                │     → Returns PyTorch Subsets     │\n                │  5. print_fold_stats() shows      │\n                │     per-class sample counts       │\n                └──────────────────────────────────┘\n```\n\n**Relationship**:\n\n* **`CMIFeDataset`** = **full data preparation engine** (raw CSV → clean, scaled, padded tensors).\n* **`CMIFoldDataset`** = **cross-validation manager** (splits the already-prepared dataset into balanced folds).\n\n---\n","metadata":{}},{"id":"17098cd8-0ae2-47b2-b67b-8f044536ac0a","cell_type":"markdown","source":"# **Model Construction**","metadata":{}},{"id":"3ca5b5be-7148-487a-8f82-3f5d191c373a","cell_type":"markdown","source":"**Classes**","metadata":{}},{"id":"ffb39b11-7ee9-4764-a506-73b900923752","cell_type":"markdown","source":"This `SEBlock` is a **Squeeze-and-Excitation** module for 1D features.\n\n**Short explanation:**\nIt learns **channel-wise attention weights** to emphasize important feature channels and suppress less useful ones.\n\n**Steps:**\n\n1. **Squeeze:** `adaptive_avg_pool1d` reduces each channel to a single value → global average per channel.\n2. **Excitation:** Two small fully connected layers (with reduction ratio) learn how important each channel is.\n3. **Scale:** A `sigmoid` gives weights in `[0, 1]` for each channel, which are multiplied back with the original `x`.\n\nEffect: Channels that help the task get boosted, and noisy channels get weakened.\n","metadata":{}},{"id":"30f4ec32-c39e-432a-85a1-d1c50c788d3f","cell_type":"code","source":"class SEBlock(nn.Module):\n    def __init__(self, channels, reduction = 8):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (B, C, L)\n        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n        return x * se                \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.043489Z","iopub.execute_input":"2025-09-06T21:58:44.043714Z","iopub.status.idle":"2025-09-06T21:58:44.061145Z","shell.execute_reply.started":"2025-09-06T21:58:44.043697Z","shell.execute_reply":"2025-09-06T21:58:44.060543Z"}},"outputs":[],"execution_count":8},{"id":"27aa1ead-7816-4f0e-9028-ad1e98ad69ca","cell_type":"markdown","source":"This `ResNetSEBlock` is basically a **1D ResNet block with a Squeeze-and-Excitation (SE) module**.\n\n**Short explanation:**\nIt applies two convolution layers with batch normalization and ReLU, recalibrates channel importance using **SEBlock**, adds a residual (shortcut) connection, and applies ReLU again.\n\n**Steps:**\n\n1. **Main path:**\n\n   * Conv1D → BN → ReLU\n   * Conv1D → BN\n   * SEBlock to apply channel attention.\n2. **Shortcut path:**\n\n   * If input/output channels differ → 1×1 Conv + BN to match dimensions.\n   * Else, pass identity directly.\n3. **Add & Activate:**\n\n   * Sum main path output and shortcut.\n   * Apply final ReLU.\n\n**Effect:**\nKeeps the benefits of residual learning (easy gradient flow, deeper networks) while also letting the network focus on **important channels** via the SE module.\n","metadata":{}},{"id":"464867e4-9c03-4e3e-89cc-6da8d1c52341","cell_type":"code","source":"class ResNetSEBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, wd = 1e-4):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, out_channels,\n                               kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.conv2 = nn.Conv1d(out_channels, out_channels,\n                               kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n        # SE\n        self.se = SEBlock(out_channels)\n        \n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n                          padding=0, bias=False),\n                nn.BatchNorm1d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x) :\n        identity = self.shortcut(x)              # (B, out, L)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)                       # (B, out, L)\n        out = out + identity\n        return self.relu(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.063625Z","iopub.execute_input":"2025-09-06T21:58:44.063818Z","iopub.status.idle":"2025-09-06T21:58:44.083950Z","shell.execute_reply.started":"2025-09-06T21:58:44.063803Z","shell.execute_reply":"2025-09-06T21:58:44.083233Z"}},"outputs":[],"execution_count":9},{"id":"3851ee71-8113-4690-8327-fa677a155b21","cell_type":"markdown","source":"This `CMIModel` is a **multi-branch deep learning model** that processes **three different sensor inputs** (IMU, thermal (thm), and time-of-flight (tof)) separately, fuses them, and uses a **BERT-based transformer** for sequence modeling before classification.\n\n---\n\n### **Short explanation**\n\n1. **Three feature extraction branches:**\n\n   * **IMU branch** → uses multiple `ResNetSEBlock`s for deep feature extraction with channel attention (SE).\n   * **THM branch** → two Conv1D + BN + ReLU + MaxPool layers.\n   * **TOF branch** → similar to THM branch.\n\n2. **Fusion:**\n\n   * After extracting features, concatenates them along the **feature dimension**.\n\n3. **BERT for sequence modeling:**\n\n   * Adds a trainable `cls_token` (like in Vision Transformers).\n   * Passes the sequence through a BERT encoder (custom config).\n   * Uses the output of the `cls_token` position as the **global representation**.\n\n4. **Classification:**\n\n   * A multi-layer fully connected classifier converts the BERT output into final class scores.\n\n---\n\n### **Data flow (forward pass)**\n\n1. **IMU input** `(B, L, imu_dim)` → `permute` to `(B, imu_dim, L)` → goes through **IMU branch** (ResNetSE + pooling + dropout) → `(B, feat_dim, reduced_L)`.\n2. **THM input** `(B, L, thm_dim)` → similar but with Conv1D layers → `(B, feat_dim, reduced_L)`.\n3. **TOF input** `(B, L, tof_dim)` → similar Conv1D layers → `(B, feat_dim, reduced_L)`.\n4. **Concat features** → `(B, reduced_L, feat_dim_total)` → transformer input format.\n5. **Add `cls_token`** and feed into **BERT** → `(B, reduced_L+1, feat_dim)`.\n6. **Take `cls_token` output** → classifier → final `(B, n_classes)` logits.\n\n---\n\n**Effect:**\nThis model combines **CNNs for local feature extraction**, **SE blocks for channel attention**, and **BERT for long-range temporal dependencies**, making it suitable for **multi-sensor sequence classification**.\n\n---","metadata":{}},{"id":"0ae66e13-b736-41dd-a724-83dcb483028a","cell_type":"code","source":"# CMIModel Class\nclass CMIModel(nn.Module):\n    def __init__(self, imu_dim, thm_dim, tof_dim, n_classes, **kwargs):\n        super().__init__()\n        self.imu_branch = nn.Sequential(\n            self.residual_se_cnn_block(imu_dim, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"],\n                                       drop=kwargs[\"imu1_dropout\"]),\n            self.residual_se_cnn_block(kwargs[\"imu1_channels\"], kwargs[\"feat_dim\"], kwargs[\"imu2_layers\"],\n                                       drop=kwargs[\"imu2_dropout\"])\n        )\n\n        self.thm_branch = nn.Sequential(\n            nn.Conv1d(thm_dim, kwargs[\"thm1_channels\"], kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm1d(kwargs[\"thm1_channels\"]),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(2, ceil_mode=True),\n            nn.Dropout(kwargs[\"thm1_dropout\"]),\n            \n            nn.Conv1d(kwargs[\"thm1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(2, ceil_mode=True),\n            nn.Dropout(kwargs[\"thm2_dropout\"])\n        )\n        \n        self.tof_branch = nn.Sequential(\n            nn.Conv1d(tof_dim, kwargs[\"tof1_channels\"], kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm1d(kwargs[\"tof1_channels\"]),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(2, ceil_mode=True),\n            nn.Dropout(kwargs[\"tof1_dropout\"]),\n            \n            nn.Conv1d(kwargs[\"tof1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(2, ceil_mode=True),\n            nn.Dropout(kwargs[\"tof2_dropout\"])\n        )\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, kwargs[\"feat_dim\"]))\n        self.bert = BertModel(BertConfig(\n            hidden_size=kwargs[\"feat_dim\"],\n            num_hidden_layers=kwargs[\"bert_layers\"],\n            num_attention_heads=kwargs[\"bert_heads\"],\n            intermediate_size=kwargs[\"feat_dim\"]*4\n        ))\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(kwargs[\"feat_dim\"], kwargs[\"cls1_channels\"], bias=False),\n            nn.BatchNorm1d(kwargs[\"cls1_channels\"]),\n            nn.ReLU(inplace=True),\n            nn.Dropout(kwargs[\"cls1_dropout\"]),\n            nn.Linear(kwargs[\"cls1_channels\"], kwargs[\"cls2_channels\"], bias=False),\n            nn.BatchNorm1d(kwargs[\"cls2_channels\"]),\n            nn.ReLU(inplace=True),\n            nn.Dropout(kwargs[\"cls2_dropout\"]),\n            nn.Linear(kwargs[\"cls2_channels\"], n_classes)\n        )\n    \n    def residual_se_cnn_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3, wd=1e-4):\n        return nn.Sequential(\n            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n            ResNetSEBlock(in_channels, out_channels, wd=wd),\n            nn.MaxPool1d(pool_size),\n            nn.Dropout(drop)\n        )\n    \n    def forward(self, imu, thm, tof):\n        imu_feat = self.imu_branch(imu.permute(0, 2, 1))\n        thm_feat = self.thm_branch(thm.permute(0, 2, 1))\n        tof_feat = self.tof_branch(tof.permute(0, 2, 1))\n        \n        bert_input = torch.cat([imu_feat, thm_feat, tof_feat], dim=-1).permute(0, 2, 1)\n        cls_token = self.cls_token.expand(bert_input.size(0), -1, -1)  # (B,1,H)\n        bert_input = torch.cat([cls_token, bert_input], dim=1)  # (B,T+1,H)\n        outputs = self.bert(inputs_embeds=bert_input)\n        pred_cls = outputs.last_hidden_state[:, 0, :]\n\n        return self.classifier(pred_cls)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.084766Z","iopub.execute_input":"2025-09-06T21:58:44.085031Z","iopub.status.idle":"2025-09-06T21:58:44.101525Z","shell.execute_reply.started":"2025-09-06T21:58:44.084989Z","shell.execute_reply":"2025-09-06T21:58:44.100766Z"}},"outputs":[],"execution_count":10},{"id":"646f7556-e9b0-4618-a439-cc2b23ba31c9","cell_type":"markdown","source":"# **Calling Dataset Creation**","metadata":{}},{"id":"a0240018-5736-496e-960d-8d89c8824e03","cell_type":"code","source":"CUDA0 = \"cuda:0\"\nseed = 0\nbatch_size = 64\nnum_workers = 4\nn_folds = 5\n\nroot_dir = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nuniverse_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n\ndeterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\ndeterministic.init_all(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.102375Z","iopub.execute_input":"2025-09-06T21:58:44.102575Z","iopub.status.idle":"2025-09-06T21:58:44.359927Z","shell.execute_reply.started":"2025-09-06T21:58:44.102554Z","shell.execute_reply":"2025-09-06T21:58:44.359027Z"}},"outputs":[],"execution_count":11},{"id":"bbfcea29-6b0e-4ad3-8671-d60cd50e3c4c","cell_type":"markdown","source":"**init_dataset() ties them together**\n\nWhen you run:\n```\n    dataset = CMIFoldDataset(..., full_dataset_function=CMIFeDataset)\n```\n\nthe sequence is:\n\n\n1. CMIFoldDataset calls CMIFeDataset(data_path, config)→ Loads and preprocesses the entire dataset once.\n2. Splits that processed dataset into n_folds (cross-validation).\n3. Prints fold stats.\n4. Returns the fold-managed dataset object.\n","metadata":{}},{"id":"4d759307-d014-4041-aae9-e925382b446b","cell_type":"markdown","source":"```\n\n CSV file (raw)\n   │\n   ▼\n CMIFeDataset (load + clean + scale + process)\n   │ processed full dataset\n   ▼\n CMIFoldDataset (split into n_folds for CV)\n   │\n   ├── fold 0: train / val\n   ├── fold 1: train / val\n   ├── ...\n   └── fold n: train / val\n\n```\n","metadata":{}},{"id":"4021ada5-af07-4a74-bb2f-2101d24d6950","cell_type":"code","source":"def init_dataset():\n    dataset_config = {\n        \"percent\": 95,\n        \"scaler_config\": StandardScaler(),\n        \"nan_ratio\": {\n            \"imu\": 0,\n            \"thm\": 0,\n            \"tof\": 0,\n        },\n        \"fbfill\": {\n            \"imu\": True,\n            \"thm\": True,\n            \"tof\": True,\n        },\n        \"one_scale\": True,\n        \"tof_raw\": True,\n        \"tof_mode\": 16,\n        \"save_precompute\": False,\n    }\n    dataset = CMIFoldDataset(universe_csv_path, dataset_config,\n                             n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset)\n    dataset.print_fold_stats()\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.360832Z","iopub.execute_input":"2025-09-06T21:58:44.361050Z","iopub.status.idle":"2025-09-06T21:58:44.366381Z","shell.execute_reply.started":"2025-09-06T21:58:44.361033Z","shell.execute_reply":"2025-09-06T21:58:44.365491Z"}},"outputs":[],"execution_count":12},{"id":"de5c79bd-efb0-4fa8-a2ad-40bd95dd7452","cell_type":"markdown","source":"**get_fold_dataset :**\n\n* Calls dataset.get_fold_datasets(fold) → returns (train_dataset, valid_dataset) for that fold.\n* Ignores training dataset (_) and keeps only the validation dataset.\n* Wraps it in a PyTorch DataLoader for batching and parallel loading:\n\n**batch_size = how many samples per batch.\nnum_workers = parallel threads for loading data.\nshuffle=False → validation order is fixed.**\n\n\n* Returns this valid_loader.\n\n","metadata":{}},{"id":"37163518-fa93-4b50-9097-268efe94eb18","cell_type":"code","source":"def get_fold_dataset(dataset, fold):\n    _, valid_dataset = dataset.get_fold_datasets(fold)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n    return valid_loader\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.367245Z","iopub.execute_input":"2025-09-06T21:58:44.367494Z","iopub.status.idle":"2025-09-06T21:58:44.387946Z","shell.execute_reply.started":"2025-09-06T21:58:44.367476Z","shell.execute_reply":"2025-09-06T21:58:44.387317Z"}},"outputs":[],"execution_count":13},{"id":"915292b3-81c5-48bd-a9ed-0f160e9eb18f","cell_type":"code","source":"dataset = init_dataset()\n\nmodel_function = CMIModel\nmodel_args = {\"feat_dim\": 500,\n              \"imu1_channels\": 219, \"imu1_dropout\": 0.2946731587132302, \"imu2_dropout\": 0.2697745571929592,\n              \"imu1_weight_decay\": 0.0014824054650601245, \"imu2_weight_decay\": 0.002742543773142381,\n              \"imu1_layers\": 0, \"imu2_layers\": 0,\n              \"thm1_channels\": 82, \"thm1_dropout\": 0.2641274454844602, \"thm2_dropout\": 0.302896343020985, \n              \"tof1_channels\": 82, \"tof1_dropout\": 0.2641274454844602, \"tof2_dropout\": 0.3028963430209852, \n              \"bert_layers\": 8, \"bert_heads\": 10,\n              \"cls1_channels\": 937, \"cls2_channels\": 303, \"cls1_dropout\": 0.2281834512100508, \"cls2_dropout\": 0.22502521933558461}\nmodel_args.update({\n    \"imu_dim\": dataset.full_dataset.imu_dim, \n    \"thm_dim\": dataset.full_dataset.thm_dim,\n    \"tof_dim\": dataset.full_dataset.tof_dim,\n    \"n_classes\": dataset.full_dataset.class_num})\nmodel_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n\nmodel_dicts = [\n    {\n        \"model_function\": model_function,\n        \"model_args\": model_args,\n        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n    } for fold in range(n_folds)\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T21:58:44.388697Z","iopub.execute_input":"2025-09-06T21:58:44.388881Z","iopub.status.idle":"2025-09-06T22:01:14.480180Z","shell.execute_reply.started":"2025-09-06T21:58:44.388867Z","shell.execute_reply":"2025-09-06T22:01:14.479359Z"}},"outputs":[{"name":"stdout","text":"Have precomputed, skip compute.\n\nCross-validation fold statistics:\n\nFold 1:\nCategory                                           Training set Validation set\nAbove ear - pull hair                              511        127       \nCheek - pinch skin                                 509        128       \nDrink from bottle/cup                              129        32        \nEyebrow - pull hair                                510        128       \nEyelash - pull hair                                512        128       \nFeel around in tray and pull out an object         129        32        \nForehead - pull hairline                           512        128       \nForehead - scratch                                 512        128       \nGlasses on/off                                     128        33        \nNeck - pinch skin                                  512        128       \nNeck - scratch                                     512        128       \nPinch knee/leg skin                                129        32        \nPull air toward your face                          381        96        \nScratch knee/leg skin                              129        32        \nText on phone                                      512        128       \nWave hello                                         382        96        \nWrite name in air                                  382        95        \nWrite name on leg                                  129        32        \n\nFold 2:\nCategory                                           Training set Validation set\nAbove ear - pull hair                              511        127       \nCheek - pinch skin                                 509        128       \nDrink from bottle/cup                              129        32        \nEyebrow - pull hair                                510        128       \nEyelash - pull hair                                512        128       \nFeel around in tray and pull out an object         129        32        \nForehead - pull hairline                           512        128       \nForehead - scratch                                 512        128       \nGlasses on/off                                     129        32        \nNeck - pinch skin                                  512        128       \nNeck - scratch                                     512        128       \nPinch knee/leg skin                                129        32        \nPull air toward your face                          381        96        \nScratch knee/leg skin                              129        32        \nText on phone                                      512        128       \nWave hello                                         382        96        \nWrite name in air                                  382        95        \nWrite name on leg                                  129        32        \n\nFold 3:\nCategory                                           Training set Validation set\nAbove ear - pull hair                              510        128       \nCheek - pinch skin                                 510        127       \nDrink from bottle/cup                              129        32        \nEyebrow - pull hair                                511        127       \nEyelash - pull hair                                512        128       \nFeel around in tray and pull out an object         129        32        \nForehead - pull hairline                           512        128       \nForehead - scratch                                 512        128       \nGlasses on/off                                     129        32        \nNeck - pinch skin                                  512        128       \nNeck - scratch                                     512        128       \nPinch knee/leg skin                                128        33        \nPull air toward your face                          382        95        \nScratch knee/leg skin                              129        32        \nText on phone                                      512        128       \nWave hello                                         382        96        \nWrite name in air                                  382        95        \nWrite name on leg                                  128        33        \n\nFold 4:\nCategory                                           Training set Validation set\nAbove ear - pull hair                              510        128       \nCheek - pinch skin                                 510        127       \nDrink from bottle/cup                              129        32        \nEyebrow - pull hair                                511        127       \nEyelash - pull hair                                512        128       \nFeel around in tray and pull out an object         128        33        \nForehead - pull hairline                           512        128       \nForehead - scratch                                 512        128       \nGlasses on/off                                     129        32        \nNeck - pinch skin                                  512        128       \nNeck - scratch                                     512        128       \nPinch knee/leg skin                                129        32        \nPull air toward your face                          382        95        \nScratch knee/leg skin                              128        33        \nText on phone                                      512        128       \nWave hello                                         383        95        \nWrite name in air                                  381        96        \nWrite name on leg                                  129        32        \n\nFold 5:\nCategory                                           Training set Validation set\nAbove ear - pull hair                              510        128       \nCheek - pinch skin                                 510        127       \nDrink from bottle/cup                              128        33        \nEyebrow - pull hair                                510        128       \nEyelash - pull hair                                512        128       \nFeel around in tray and pull out an object         129        32        \nForehead - pull hairline                           512        128       \nForehead - scratch                                 512        128       \nGlasses on/off                                     129        32        \nNeck - pinch skin                                  512        128       \nNeck - scratch                                     512        128       \nPinch knee/leg skin                                129        32        \nPull air toward your face                          382        95        \nScratch knee/leg skin                              129        32        \nText on phone                                      512        128       \nWave hello                                         383        95        \nWrite name in air                                  381        96        \nWrite name on leg                                  129        32        \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"id":"a67efa8f-456d-475b-a677-5990d8e2e72c","cell_type":"code","source":"models2 = list()\nfor model_dict in model_dicts:\n    model_function = model_dict[\"model_function\"]\n    model_args = model_dict[\"model_args\"]\n    model_path = model_dict[\"model_path\"]\n    model = model_function(**model_args).to(CUDA0)\n    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k,v in torch.load(model_path).items()}\n    model.load_state_dict(state_dict)\n    model = model.eval()\n    models2.append(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:01:14.481121Z","iopub.execute_input":"2025-09-06T22:01:14.481448Z","iopub.status.idle":"2025-09-06T22:01:24.827038Z","shell.execute_reply.started":"2025-09-06T22:01:14.481424Z","shell.execute_reply":"2025-09-06T22:01:24.826165Z"}},"outputs":[],"execution_count":15},{"id":"455d1b18-ad97-40b7-8c37-95338a45e9d7","cell_type":"code","source":"metric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n\nmetric = metric_package.Metric()\nimu_only_metric = metric_package.Metric()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:01:24.827895Z","iopub.execute_input":"2025-09-06T22:01:24.828366Z","iopub.status.idle":"2025-09-06T22:01:25.059536Z","shell.execute_reply.started":"2025-09-06T22:01:24.828340Z","shell.execute_reply":"2025-09-06T22:01:25.058815Z"}},"outputs":[],"execution_count":16},{"id":"e4c3fb04-5076-41a7-9932-40abd85ffc90","cell_type":"code","source":"def to_cuda(*tensors):\n    return [tensor.to(CUDA0) for tensor in tensors]\n\ndef predict_fold(model, imu, thm, tof):\n    pred = model(imu, thm, tof)\n    return pred\n\ndef valid(model, valid_bar):\n    with torch.no_grad():\n        for imu, thm, tof, y in valid_bar:\n            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n            with autocast(device_type='cuda', dtype=torch.bfloat16): \n                logits = predict_fold(model, imu, thm, tof)\n            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n            with autocast(device_type='cuda', dtype=torch.bfloat16): \n                logits = model(imu, thm, tof)\n            imu_only_metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n\ndef avg_predict(models, imu, thm, tof):\n    outputs = []\n    with autocast(device_type='cuda'):\n        for model in models:\n            logits = model(imu, thm, tof)\n        outputs.append(logits)\n    return torch.mean(torch.stack(outputs), dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:01:25.060458Z","iopub.execute_input":"2025-09-06T22:01:25.061148Z","iopub.status.idle":"2025-09-06T22:01:25.067855Z","shell.execute_reply.started":"2025-09-06T22:01:25.061099Z","shell.execute_reply":"2025-09-06T22:01:25.067171Z"}},"outputs":[],"execution_count":17},{"id":"b0e9c1fa","cell_type":"code","source":"def predict2(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n    with torch.no_grad():\n        imu, thm, tof = to_cuda(imu, thm, tof)\n        logits = avg_predict(models2, imu, thm, tof)\n        probabilities = F.softmax(logits, dim=1).cpu().numpy()\n    return probabilities","metadata":{"_cell_guid":"4ec7ba83-bd38-4944-acc5-c569d0b22991","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"9ee5932b-ffff-47c4-b5b3-abb1efdfab5c","execution":{"iopub.status.busy":"2025-09-06T22:01:25.068696Z","iopub.execute_input":"2025-09-06T22:01:25.068921Z","iopub.status.idle":"2025-09-06T22:01:25.087312Z","shell.execute_reply.started":"2025-09-06T22:01:25.068905Z","shell.execute_reply":"2025-09-06T22:01:25.086687Z"},"papermill":{"duration":0.012183,"end_time":"2025-09-01T14:27:24.159983","exception":false,"start_time":"2025-09-01T14:27:24.147800","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"id":"e8e9f998","cell_type":"code","source":"def enumerate_weights():\n    list_A = [0.500, 0.505, 0.510, 0.515, 0.520, 0.525, 0.530, 0.535]\n    list_B = [0.174, 0.180, 0.185, 0.190, 0.195, 0.200, 0.205, 0.210]\n    \n    wts = []\n    for A in list_A:\n        for B in list_B:\n            wts.append({'A':A, 'B':B, 'C':1-(A+B)})\n    return wts","metadata":{"execution":{"iopub.status.busy":"2025-09-06T22:01:25.088086Z","iopub.execute_input":"2025-09-06T22:01:25.088393Z","iopub.status.idle":"2025-09-06T22:01:25.101899Z","shell.execute_reply.started":"2025-09-06T22:01:25.088359Z","shell.execute_reply":"2025-09-06T22:01:25.101364Z"},"papermill":{"duration":0.011873,"end_time":"2025-09-01T14:27:30.317078","exception":false,"start_time":"2025-09-01T14:27:30.305205","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"id":"73c0f40f","cell_type":"code","source":"def find_max_v(one_avg_pred):\n    max_val = -float('inf')\n    for val in one_avg_pred:\n        if val > max_val:\n            max_val = val\n    return max_val\n    \n\ndef distanceTo(centr_mass, points):\n    return math.sqrt(sum([(cm - p) ** 2 for cm, p in zip(centr_mass, points)]))\n\n","metadata":{"execution":{"iopub.status.busy":"2025-09-06T22:01:25.102539Z","iopub.execute_input":"2025-09-06T22:01:25.102710Z","iopub.status.idle":"2025-09-06T22:01:25.118789Z","shell.execute_reply.started":"2025-09-06T22:01:25.102697Z","shell.execute_reply":"2025-09-06T22:01:25.118086Z"},"papermill":{"duration":0.021771,"end_time":"2025-09-01T14:27:30.400286","exception":false,"start_time":"2025-09-01T14:27:30.378515","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"id":"80410a44-8207-4a87-91f2-bde207b8d3b1","cell_type":"code","source":"def predict(sequence, demographics):\n    # Only use model 2\n    pred2 = predict2(sequence, demographics)[0]\n\n    # Take the prediction directly\n    avg_pred = np.asarray(pred2)\n\n    return dataset.le.classes_[avg_pred.argmax()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T22:01:25.119613Z","iopub.execute_input":"2025-09-06T22:01:25.119871Z","iopub.status.idle":"2025-09-06T22:01:25.139431Z","shell.execute_reply.started":"2025-09-06T22:01:25.119850Z","shell.execute_reply":"2025-09-06T22:01:25.138881Z"}},"outputs":[],"execution_count":21},{"id":"0e069820-2935-4fb9-800c-b37721c673da","cell_type":"markdown","source":"# Submission","metadata":{}},{"id":"55c2831c","cell_type":"code","source":"import kaggle_evaluation.cmi_inference_server\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )\n\nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(pd.read_parquet(\"submission.parquet\"))","metadata":{"_cell_guid":"d950fdf3-997f-4502-92ff-83a5d2014717","_kg_hide-output":true,"_uuid":"46f1f772-fc94-4243-b3bd-d7b91e64b825","execution":{"iopub.status.busy":"2025-09-06T22:01:25.140025Z","iopub.execute_input":"2025-09-06T22:01:25.140247Z","iopub.status.idle":"2025-09-06T22:01:29.818953Z","shell.execute_reply.started":"2025-09-06T22:01:25.140232Z","shell.execute_reply":"2025-09-06T22:01:29.818177Z"},"papermill":{"duration":59.477554,"end_time":"2025-09-01T14:28:29.883697","exception":false,"start_time":"2025-09-01T14:27:30.406143","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"  sequence_id              gesture\n0  SEQ_000011  Eyelash - pull hair\n1  SEQ_000001  Eyebrow - pull hair\n","output_type":"stream"}],"execution_count":22}]}